{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evgenia Stamati, Charalampos Kaidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code sequence bellow we have created two function to calculate the out-degree-sequence on local files and on S3 based buckets.\n",
    "\n",
    "Calling the program with parameter \"local\" computes the result on files located on local filesystem by using the compute_local function.\n",
    "\n",
    "Calling the program with parameter \"s3\" computes the result on buckets located on an S3 filesystem using the boto3 module. The implementing function is compute_s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load partA/assignment.py\n",
    "#!/usr/local/bin/python2.7\n",
    "'''\n",
    "Part A -- Calculate out degree sequence of RDF graph\n",
    "\n",
    "@author:     Charalampos Kaidos, Evgenia Stamati\n",
    "\n",
    "@copyright:  2016 Charalampos Kaidos, Evgenia Stamati. All rights reserved.\n",
    "\n",
    "@contact:    kaidosc@aueb.gr\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "import boto3\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from argparse import RawDescriptionHelpFormatter\n",
    "from inspect import ArgSpec\n",
    "\n",
    "__all__ = []\n",
    "__version__ = 0.1\n",
    "__date__ = '2016-04-10'\n",
    "__updated__ = '2016-04-18'\n",
    "\n",
    "DEBUG = 0\n",
    "TESTRUN = 0\n",
    "PROFILE = 0\n",
    "\n",
    "def plot_figs(out_degree_sequence, verbose):\n",
    "    \"\"\"Plots figures of out-degree-sequence\n",
    "\n",
    "    Creates 2 figures;\n",
    "    first figure plots the out-degree-sequence\n",
    "    second figure plots the log values of out-degree-sequence\n",
    "\n",
    "    Both figures are saved on filesystem with names test.png and testlog.png\n",
    "    respectively\n",
    "    \"\"\"\n",
    "    filename = 'out-degree-sequence.png'\n",
    "    filename_log = 'log-out-degree-sequence.png'\n",
    "\n",
    "    out_degree = out_degree_sequence.keys()\n",
    "    nodes = out_degree_sequence.values()\n",
    "\n",
    "    if(verbose):\n",
    "        print 'Plotting out-degree-sequence to file ' + filename\n",
    "    matplotlib.pyplot.scatter(out_degree,nodes)\n",
    "    matplotlib.pyplot.suptitle('out degree sequence scatterplot', fontsize=20)\n",
    "    matplotlib.pyplot.xlabel('out degree', fontsize=18)\n",
    "    matplotlib.pyplot.ylabel('out degree sequence', fontsize=16)\n",
    "    matplotlib.pyplot.grid(True)\n",
    "    matplotlib.pyplot.savefig(filename)\n",
    "    matplotlib.pyplot.close()\n",
    "\n",
    "    x = numpy.log(out_degree)\n",
    "    y = numpy.log(nodes)\n",
    "\n",
    "    if(verbose):\n",
    "        print 'Plotting log-out-degree-sequence to file ' + filename_log\n",
    "    matplotlib.pyplot.scatter(x,y)\n",
    "    matplotlib.pyplot.suptitle('log of out degree sequence scatterplot', fontsize=20)\n",
    "    matplotlib.pyplot.xlabel('log out degree', fontsize=18)\n",
    "    matplotlib.pyplot.ylabel('log out degree sequence', fontsize=16)\n",
    "    matplotlib.pyplot.grid(True)\n",
    "    matplotlib.pyplot.savefig(filename_log)\n",
    "\n",
    "def save_to_file(out_degree_sequence, verbose):\n",
    "    \"\"\"Saves out-degree-sequence to file\n",
    "    \"\"\"\n",
    "    filename = 'out-degree-sequence'\n",
    "    if(verbose):\n",
    "        print 'Saving out-degree-sequence to file ' + filename\n",
    "    with open(filename, 'w') as fout:\n",
    "        for key,value in out_degree_sequence.iteritems():\n",
    "            fout.write(str(key) + '\\t' + str(value) + '\\n')\n",
    "\n",
    "def compute_local(args):\n",
    "    path = args.localpath\n",
    "    verbose = args.verbose\n",
    "    \n",
    "    file_list = []\n",
    "    if os.path.isdir(path):\n",
    "        if(verbose):\n",
    "            print 'Given path \"' + path + '\" is a directory. Will read all files in it.'\n",
    "        file_list = [os.path.join(path,file) for file in os.listdir(path) if os.path.isfile(os.path.join(path,file))]\n",
    "    elif os.path.isfile(path):\n",
    "        if(verbose):\n",
    "            print 'Given path \"' + path + '\" is a file.'\n",
    "        file_list.append(path)\n",
    "    else:\n",
    "        print('Given path: \"%s\" is not a directory nor a file' % path)\n",
    "        raise os.error\n",
    "    \n",
    "    subject_list = Counter()\n",
    "    \n",
    "    for fileName in file_list:\n",
    "        if(verbose):\n",
    "            print 'Reading file: ' + filename\n",
    "        count = 0\n",
    "        with open( fileName, \"r\" ) as fin:\n",
    "            for line in fin:\n",
    "                count += 1\n",
    "                key = line.split(' ', 1)[0]\n",
    "                subject_list[key] += 1\n",
    "        print 'File: ' + filename + ' complete: ' + str(count) + ' records processed'\n",
    "\n",
    "    if(verbose):\n",
    "        print 'All files complete: ' + str(len(subject_list)) + ' nodes processed'\n",
    "\n",
    "    out_degree_sequence = Counter(subject_list.itervalues())\n",
    "\n",
    "    if(verbose):\n",
    "        print 'Out-degree-sequence calculated: ' + str(len(out_degree_sequence)) + ' entries'\n",
    "    \n",
    "    plot_figs(out_degree_sequence, verbose)\n",
    "    save_to_file(out_degree_sequence, verbose)\n",
    "\n",
    "def compute_s3(args):\n",
    "    verbose = args.verbose\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    bucket_name = args.s3bucket\n",
    "    if(verbose):\n",
    "        print 'Bucket selected: ' + bucket_name\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    key_name = args.s3key\n",
    "    if(verbose):\n",
    "        print 'Key selected: ' + key_name\n",
    "    \n",
    "    subject_list = Counter()\n",
    "    \n",
    "    if key_name == '*':\n",
    "        for object in bucket.objects.all():\n",
    "            if(verbose):\n",
    "                print 'Reading object: ' + object.key\n",
    "            leftover = ''\n",
    "            data = object.get()\n",
    "            chunk = data['Body'].read(1024*1024)\n",
    "            count = 0\n",
    "            while chunk:\n",
    "                lines = chunk.splitlines(True)\n",
    "                for line in lines:\n",
    "                    if(line[-1] == '\\n'):\n",
    "                        count += 1\n",
    "                        key = line.split(' ', 1)[0]\n",
    "                        subject_list[key] += 1\n",
    "                    else:\n",
    "                        leftover = line\n",
    "                chunk = data['Body'].read(1024*1024*100)\n",
    "                if (chunk == '' and leftover != ''):\n",
    "                    chunk += '\\n'\n",
    "                chunk = leftover + chunk\n",
    "                leftover = ''\n",
    "            print 'Object ' + object.key + ' complete: ' + str(count) + ' records processed'\n",
    "    else:\n",
    "        leftover = ''\n",
    "        object = s3.Object(bucket_name, args.s3key)\n",
    "        if(verbose):\n",
    "            print 'Reading object: ' + object.key\n",
    "        data = object.get()\n",
    "        chunk = data['Body'].read(1024*1024)\n",
    "        if(verbose):\n",
    "            print 'Read chunk'\n",
    "        count = 0\n",
    "        while chunk:\n",
    "            lines = chunk.splitlines(True)\n",
    "            for line in lines:\n",
    "                if(line[-1] == '\\n'):\n",
    "                    count += 1\n",
    "                    key = line.split(' ', 1)[0]\n",
    "                    subject_list[key] += 1\n",
    "                else:\n",
    "                    leftover = line\n",
    "            chunk = data['Body'].read(1024*1024*100)\n",
    "            if (chunk == '' and leftover != ''):\n",
    "                chunk += '\\n'\n",
    "            chunk = leftover + chunk\n",
    "            leftover = ''\n",
    "        print 'Object ' + object.key + ' complete: ' + str(count) + ' records processed'\n",
    "\n",
    "    if(verbose):\n",
    "        print 'All files complete: ' + str(len(subject_list)) + ' nodes processed'\n",
    "\n",
    "    out_degree_sequence = Counter(subject_list.itervalues())\n",
    "\n",
    "    if(verbose):\n",
    "        print 'Out-degree-sequence calculated: ' + str(len(out_degree_sequence)) + ' entries'\n",
    "\n",
    "    plot_figs(out_degree_sequence, verbose)\n",
    "    save_to_file(out_degree_sequence, verbose)\n",
    "\n",
    "class CLIError(Exception):\n",
    "    '''Generic exception to raise and log different fatal errors.'''\n",
    "    def __init__(self, msg):\n",
    "        super(CLIError).__init__(type(self))\n",
    "        self.msg = \"E: %s\" % msg\n",
    "    def __str__(self):\n",
    "        return self.msg\n",
    "    def __unicode__(self):\n",
    "        return self.msg\n",
    "\n",
    "def main(argv=None): # IGNORE:C0111\n",
    "    '''Command line options.'''\n",
    "\n",
    "    if argv is None:\n",
    "        argv = sys.argv\n",
    "    else:\n",
    "        sys.argv.extend(argv)\n",
    "\n",
    "    program_name = os.path.basename(sys.argv[0])\n",
    "    program_version = \"v%s\" % __version__\n",
    "    program_build_date = str(__updated__)\n",
    "    program_version_message = '%%(prog)s %s (%s)' % (program_version, program_build_date)\n",
    "    program_shortdesc = __import__('__main__').__doc__.split(\"\\n\")[1]\n",
    "    program_license = '''%s\n",
    "\n",
    "  Created by user_name on %s.\n",
    "  Copyright 2016 organization_name. All rights reserved.\n",
    "\n",
    "  Licensed under the Apache License 2.0\n",
    "  http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Distributed on an \"AS IS\" basis without warranties\n",
    "  or conditions of any kind, either express or implied.\n",
    "\n",
    "USAGE\n",
    "''' % (program_shortdesc, str(__date__))\n",
    "\n",
    "    try:\n",
    "        # Setup argument parser\n",
    "        parser = ArgumentParser(description=program_license, formatter_class=RawDescriptionHelpFormatter)\n",
    "        parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"count\", help=\"set verbosity level [default: %(default)s]\")\n",
    "        parser.add_argument('-V', '--version', action='version', version=program_version_message)\n",
    "        \n",
    "        subparsers = parser.add_subparsers(dest='fs')\n",
    "        parser_s3 = subparsers.add_parser('s3', help='Calculate on data stored on S3 cloud')\n",
    "        parser_local = subparsers.add_parser('local', help='Calculate on data stored on a local filesystem')\n",
    "        \n",
    "        parser_s3.add_argument(dest=\"s3bucket\", help='paths to folder(s) with source file(s) [default: %(default)s]', metavar='bucket')\n",
    "        parser_s3.add_argument(dest=\"s3key\", help='paths to folder(s) with source file(s) [default: %(default)s]', metavar='key')\n",
    "        \n",
    "        parser_local.add_argument(dest=\"localpath\", help=\"paths to folder(s) with source file(s) [default: %(default)s]\", metavar=\"path\")\n",
    "\n",
    "        # Process arguments\n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        verbose = args.verbose\n",
    "        if verbose > 0:\n",
    "            print(\"Verbose mode on\")\n",
    "        \n",
    "        startTime = datetime.now()\n",
    "        if args.fs == 's3':\n",
    "            compute_s3(args)\n",
    "        elif args.fs == 'local':\n",
    "            compute_local(args)\n",
    "        print (datetime.now() - startTime)\n",
    "\n",
    "        return 0\n",
    "    except KeyboardInterrupt:\n",
    "        ### handle keyboard interrupt ###\n",
    "        return 0\n",
    "    except Exception, e:\n",
    "        if DEBUG or TESTRUN:\n",
    "            raise(e)\n",
    "        indent = len(program_name) * \" \"\n",
    "        sys.stderr.write(program_name + \": \" + repr(e) + \"\\n\")\n",
    "        sys.stderr.write(indent + \"  for help use --help\")\n",
    "        return 2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if DEBUG:\n",
    "        sys.argv.append(\"-h\")\n",
    "        sys.argv.append(\"-v\")\n",
    "        sys.argv.append(\"-r\")\n",
    "    if TESTRUN:\n",
    "        import doctest\n",
    "        doctest.testmod()\n",
    "    if PROFILE:\n",
    "        import cProfile\n",
    "        import pstats\n",
    "        profile_filename = 'assignment_profile.txt'\n",
    "        cProfile.run('main()', profile_filename)\n",
    "        statsfile = open(\"profile_stats.txt\", \"wb\")\n",
    "        p = pstats.Stats(profile_filename, stream=statsfile)\n",
    "        stats = p.strip_dirs().sort_stats('cumulative')\n",
    "        stats.print_stats()\n",
    "        statsfile.close()\n",
    "        sys.exit(0)\n",
    "    sys.exit(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the program on the AWS server in order to have somewhat comparable results with the hadoop executions. The s3 filesystem is remote and trementously affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(env)[user2@ip-172-31-8-18 partA]$ python assignment.py -v s3 hw2test \"*\"\n",
    "Verbose mode on\n",
    "Bucket selected: hw2test\n",
    "Key selected: *\n",
    "Reading object: hw2-test-data.txt\n",
    "Object hw2-test-data.txt complete: 30 records processed\n",
    "All files complete: 8 nodes processed\n",
    "Out-degree-sequence calculated: 4 entries\n",
    "Plotting out-degree-sequence to file out-degree-sequence.png\n",
    "Plotting log-out-degree-sequence to file log-out-degree-sequence.png\n",
    "Saving out-degree-sequence to file out-degree-sequence\n",
    "0:00:01.075258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the test data the program calculates the out-degree-sequence in about 1 second. The sequence table is in the following cell.\n",
    "\n",
    "<img src=\"partA/out-degree-sequence-test.png\">\n",
    "\n",
    "<img src=\"partA/log-out-degree-sequence-test.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq\tNodes\n",
      "1\t2\n",
      "3\t3\n",
      "5\t1\n",
      "7\t2\n"
     ]
    }
   ],
   "source": [
    "resultTest = {}\n",
    "with open('partA/out-degree-sequence-test', 'r') as fin:\n",
    "    for line in fin:\n",
    "        key, value = line.split()\n",
    "        resultTest[key] = value\n",
    "\n",
    "print 'Seq\\tNodes'\n",
    "for key,value in resultTest.items():\n",
    "    print str(key) + '\\t' + str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the same program on all files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    (env)[user2@ip-172-31-8-18 partA]$ python assignment.py -v s3 hw2storage \"*\"\n",
    "    Verbose mode on\n",
    "    Bucket selected: hw2storage\n",
    "    Key selected: *\n",
    "    Reading object: hw2-rdf-2016_1\n",
    "    Object hw2-rdf-2016_1 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_10\n",
    "    Object hw2-rdf-2016_10 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_11\n",
    "    Object hw2-rdf-2016_11 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_12\n",
    "    Object hw2-rdf-2016_12 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_13\n",
    "    Object hw2-rdf-2016_13 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_14\n",
    "    Object hw2-rdf-2016_14 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_15\n",
    "    Object hw2-rdf-2016_15 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_16\n",
    "    Object hw2-rdf-2016_16 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_17\n",
    "    Object hw2-rdf-2016_17 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_18\n",
    "    Object hw2-rdf-2016_18 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_19\n",
    "    Object hw2-rdf-2016_19 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_2\n",
    "    Object hw2-rdf-2016_2 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_20\n",
    "    Object hw2-rdf-2016_20 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_21\n",
    "    Object hw2-rdf-2016_21 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_22\n",
    "    Object hw2-rdf-2016_22 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_23\n",
    "    Object hw2-rdf-2016_23 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_24\n",
    "    Object hw2-rdf-2016_24 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_25\n",
    "    Object hw2-rdf-2016_25 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_26\n",
    "    Object hw2-rdf-2016_26 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_27\n",
    "    Object hw2-rdf-2016_27 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_28\n",
    "    Object hw2-rdf-2016_28 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_29\n",
    "    Object hw2-rdf-2016_29 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_3\n",
    "    Object hw2-rdf-2016_3 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_30\n",
    "    Object hw2-rdf-2016_30 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_31\n",
    "    Object hw2-rdf-2016_31 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_32\n",
    "    Object hw2-rdf-2016_32 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_33\n",
    "    Object hw2-rdf-2016_33 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_34\n",
    "    Object hw2-rdf-2016_34 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_35\n",
    "    Object hw2-rdf-2016_35 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_36\n",
    "    Object hw2-rdf-2016_36 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_37\n",
    "    Object hw2-rdf-2016_37 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_38\n",
    "    Object hw2-rdf-2016_38 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_39\n",
    "    Object hw2-rdf-2016_39 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_4\n",
    "    Object hw2-rdf-2016_4 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_40\n",
    "    Object hw2-rdf-2016_40 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_41\n",
    "    Object hw2-rdf-2016_41 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_42\n",
    "    Object hw2-rdf-2016_42 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_43\n",
    "    Object hw2-rdf-2016_43 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_44\n",
    "    Object hw2-rdf-2016_44 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_45\n",
    "    Object hw2-rdf-2016_45 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_46\n",
    "    Object hw2-rdf-2016_46 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_5\n",
    "    Object hw2-rdf-2016_5 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_6\n",
    "    Object hw2-rdf-2016_6 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_7\n",
    "    Object hw2-rdf-2016_7 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_8\n",
    "    Object hw2-rdf-2016_8 complete: 10000000 records processed\n",
    "    Reading object: hw2-rdf-2016_9\n",
    "    Object hw2-rdf-2016_9 complete: 10000000 records processed\n",
    "    All files complete: 11498443 nodes processed\n",
    "    Out-degree-sequence calculated: 1250 entries\n",
    "    Plotting out-degree-sequence to file out-degree-sequence.png\n",
    "    Plotting log-out-degree-sequence to file log-out-degree-sequence.png\n",
    "    Saving out-degree-sequence to file out-degree-sequence\n",
    "    1:03:05.548768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution took more than 1 hour. This is mainly due to the remote filesystem. When executing on local filesystem, on PCs less powerfull than the AWS server, the execution time was less than 17 minutes.\n",
    "\n",
    "The plots are bellow:\n",
    "<img src=\"partA/out-degree-sequence.png\">\n",
    "\n",
    "<img src=\"partA/log-out-degree-sequence.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fianlly on the file out-degree-sequence is the result of the processing. We will load it to compare it to the result of the other execution tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultA = {}\n",
    "with open('partA/out-degree-sequence', 'r') as fin:\n",
    "    for line in fin:\n",
    "        key, value = line.split()\n",
    "        resultA[key] = value\n",
    "\n",
    "#print 'Seq\\tNodes'\n",
    "#for key,value in resultA.items():\n",
    "#    print str(key) + '\\t' + str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hadoop implementation we have designed a solution with 2 chained map-reduce jobs.\n",
    "The first job calculates the occurences of each subject. The map job counts each subject and the reduce sums all counts. The outcome is a two column table with the subject on the first column and its frequency on the second.\n",
    "This table is the input to the second job. The second mapper counts the frequencies of the first job; that is, how many subjects have a frequency of 1, how many have a frequency of 2 etc. Then the reducer sums all these and produces the out-degree-sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper of the first job is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load partB/mapper_out_degree.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    " \n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key = line.split(' ')\n",
    "    key_value = key[0]\n",
    "    print >>sys.stdout, \"%s\\t%s\" % (key_value, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer of the first job is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load partB/reducer_out_degree.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "total = 0\n",
    "prev_key = False\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    data = line.split('\\t')\n",
    "    curr_key = data[0]\n",
    "    count = int(data[1])\n",
    "\n",
    "    if prev_key and curr_key != prev_key:\n",
    "        print >>sys.stdout, \"%s\\t%i\" % (prev_key, total)\n",
    "        prev_key = curr_key\n",
    "        total = count\n",
    "    #same key; accumulate sum\n",
    "    else:\n",
    "        prev_key = curr_key\n",
    "        total += count\n",
    "\n",
    "# emit last key\n",
    "if prev_key:\n",
    "    print >>sys.stdout, \"%s\\t%i\" % (prev_key, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper of the second job is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load partB/mapper_out_sequece.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    " \n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key = line.split('\\t')\n",
    "    value = key[1]\n",
    "    #for word in key_value:\n",
    "    print >>sys.stdout, \"%s\\t%s\" % (value, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer of the second job is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load partB/reducer_out_sequece.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "total = 0\n",
    "prev_key = False\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    data = line.split('\\t')\n",
    "    curr_key = data[0]\n",
    "    count = int(data[1])\n",
    "\n",
    "    if prev_key and curr_key != prev_key:\n",
    "        print >>sys.stdout, \"%s\\t%i\" % (prev_key, total)\n",
    "        prev_key = curr_key\n",
    "        total = count\n",
    "     # same key; accumulate sum\n",
    "    else:\n",
    "        prev_key = curr_key\n",
    "        total += count\n",
    "\n",
    "# emit last key\n",
    "if prev_key:\n",
    "    print >>sys.stdout, \"%s\\t%i\" % (prev_key, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the first job by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.1-amzn-1.jar -file partB/mapper_out_degree.py -mapper 'mapper_out_degree.py' -file partB/reducer_out_degree.py -reducer 'reducer_out_degree.py' -input s3n://hw2storage/ -output /user/user2/mapreduce/output &> map1.out &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard output is stored in map1.out. From that we paste here the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    16/04/15 09:38:31 INFO mapreduce.Job: Counters: 54\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=1721693469\n",
    "                FILE: Number of bytes written=4152486533\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=126879\n",
    "                HDFS: Number of bytes written=827765180\n",
    "                HDFS: Number of read operations=3043\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=10\n",
    "                S3N: Number of bytes read=100127899750\n",
    "                S3N: Number of bytes written=0\n",
    "                S3N: Number of read operations=0\n",
    "                S3N: Number of large read operations=0\n",
    "                S3N: Number of write operations=0\n",
    "        Job Counters \n",
    "                Launched map tasks=1514\n",
    "                Launched reduce tasks=5\n",
    "                Data-local map tasks=1514\n",
    "                Total time spent by all maps in occupied slots (ms)=1464006840\n",
    "                Total time spent by all reduces in occupied slots (ms)=727183104\n",
    "                Total time spent by all map tasks (ms)=61000285\n",
    "                Total time spent by all reduce tasks (ms)=15149648\n",
    "                Total vcore-seconds taken by all map tasks=61000285\n",
    "                Total vcore-seconds taken by all reduce tasks=15149648\n",
    "                Total megabyte-seconds taken by all map tasks=46848218880\n",
    "                Total megabyte-seconds taken by all reduce tasks=23269859328\n",
    "        Map-Reduce Framework\n",
    "                Map input records=460000000\n",
    "                Map output records=460000000\n",
    "                Map output bytes=28950311440\n",
    "                Map output materialized bytes=2239145033\n",
    "                Input split bytes=126879\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=11498443\n",
    "                Reduce shuffle bytes=2239145033\n",
    "                Reduce input records=460000000\n",
    "                Reduce output records=11498443\n",
    "                Spilled Records=920000000\n",
    "                Shuffled Maps =7570\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=7570\n",
    "                GC time elapsed (ms)=601687\n",
    "                CPU time spent (ms)=21537560\n",
    "                Physical memory (bytes) snapshot=764612165632\n",
    "                Virtual memory (bytes) snapshot=2058085785600\n",
    "                Total committed heap usage (bytes)=682084532224\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters \n",
    "                Bytes Read=100127899750\n",
    "        File Output Format Counters \n",
    "                Bytes Written=827765180\n",
    "    16/04/15 09:38:31 INFO streaming.StreamJob: Output directory: /user/user2/mapreduce/output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the second job by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.1-amzn-1.jar -file partB/mapper_out_sequece.py -mapper 'mapper_out_sequece.py' -file partB/reducer_out_sequece.py -reducer 'reducer_out_sequece.py' -input /user/user2/mapreduce/output -output /user/user2/mapreduce/output2 &> map2.out&"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard output is stored in map2.out. From that we paste here the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    16/04/16 14:31:49 INFO mapreduce.Job: Counters: 51\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=3412121\n",
    "                FILE: Number of bytes written=9385708\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=828201995\n",
    "                HDFS: Number of bytes written=8652\n",
    "                HDFS: Number of read operations=60\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=10\n",
    "        Job Counters \n",
    "                Killed reduce tasks=1\n",
    "                Launched map tasks=15\n",
    "                Launched reduce tasks=6\n",
    "                Data-local map tasks=14\n",
    "                Rack-local map tasks=1\n",
    "                Total time spent by all maps in occupied slots (ms)=13452192\n",
    "                Total time spent by all reduces in occupied slots (ms)=7820160\n",
    "                Total time spent by all map tasks (ms)=560508\n",
    "                Total time spent by all reduce tasks (ms)=162920\n",
    "                Total vcore-seconds taken by all map tasks=560508\n",
    "                Total vcore-seconds taken by all reduce tasks=162920\n",
    "                Total megabyte-seconds taken by all map tasks=430470144\n",
    "                Total megabyte-seconds taken by all reduce tasks=250245120\n",
    "        Map-Reduce Framework\n",
    "                Map input records=11498443\n",
    "                Map output records=11498443\n",
    "                Map output bytes=49180641\n",
    "                Map output materialized bytes=3449584\n",
    "                Input split bytes=2160\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=1250\n",
    "                Reduce shuffle bytes=3449584\n",
    "                Reduce input records=11498443\n",
    "                Reduce output records=1250\n",
    "                Spilled Records=22996886\n",
    "                Shuffled Maps =75\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=75\n",
    "                GC time elapsed (ms)=5717\n",
    "                CPU time spent (ms)=149040\n",
    "                Physical memory (bytes) snapshot=7627571200\n",
    "                Virtual memory (bytes) snapshot=29847506944\n",
    "                Total committed heap usage (bytes)=7432830976\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters \n",
    "                Bytes Read=828199835\n",
    "        File Output Format Counters \n",
    "                Bytes Written=8652\n",
    "    16/04/16 14:31:49 INFO streaming.StreamJob: Output directory: /user/user2/mapreduce/output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the total CPU time of the 2 jobs we get a total CPU execution time of 21,686,600 ms; that is about 6 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the job is on the HDFS. We have brought it to the local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are equal: True\n"
     ]
    }
   ],
   "source": [
    "resultB = {}\n",
    "path = 'partB/output2'\n",
    "file_list = [os.path.join(path,file) for file in os.listdir(path) if os.path.isfile(os.path.join(path,file)) and file != '_SUCCESS']\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as fin:\n",
    "        for line in fin:\n",
    "            key, value = line.split()\n",
    "            resultB[key] = value\n",
    "            \n",
    "equal = False\n",
    "if(resultA == resultB):\n",
    "    equal = True\n",
    "print 'Results are equal: ' + str(equal)\n",
    "#print 'Seq\\tNodes'\n",
    "#for key,value in result.items():\n",
    "#    print str(key) + '\\t' + str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results of the hadoop execution are the same as the results of the execution in Part A. Thus we don't need to plot the graphs again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig is a high level language for hadoop. It allows to abstract much of the low level programming tasks and provides and SQL-like api to create queries that will be executed on hadoop.\n",
    "\n",
    "We have written a pig script that will to the exact same process as the python script in Part A and the hadoop scripts in Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load partC/assignment.pig\n",
    "register s3n://hw2files/myudfs.jar\n",
    "raw = LOAD  's3n://hw2storage/*' USING TextLoader as (line:chararray);\n",
    "ntriples = foreach raw generate FLATTEN(myudfs.RDFSplit3(line)) as (subject:chararray,predicate:chararray,object:chararray); \n",
    "subject_groups = group ntriples by subject;\n",
    "subject_frequencies = foreach subject_groups generate group, COUNT(ntriples) as out_degree;\n",
    "out_degree_groups = group subject_frequencies by out_degree;\n",
    "out_degree_sequence = foreach out_degree_groups generate group, COUNT(subject_frequencies);\n",
    "store out_degree_sequence into '/user/user2/out_degree_sequence' using PigStorage();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created a small wrapper script to execute the process in the background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load partC/exec_pig.sh\n",
    "#!/bin/bash\n",
    "\n",
    "START_TIME=$(date +\"%s\")\n",
    "\n",
    "pig assignment.pig &> exec_pig.log\n",
    "\n",
    "END_TIME=$(date +\"%s\") \n",
    "DIFF=$(($END_TIME-$START_TIME))\n",
    "echo \"$(($DIFF / 60)) minutes and $(($DIFF % 60)) seconds.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exec_pig.log is the standard output of the program. Here is an extract:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    16/04/10 17:08:32 INFO mapreduce.SimplePigStats: Script Statistics: \n",
    "\n",
    "    HadoopVersion   PigVersion      UserId  StartedAt       FinishedAt      Features\n",
    "    2.7.1-amzn-1    0.14.0-amzn-0   user2   2016-04-10 13:46:36     2016-04-10 17:08:32     GROUP_BY\n",
    "\n",
    "    Success!\n",
    "\n",
    "    Job Stats (time in seconds):\n",
    "    JobId   Maps    Reduces MaxMapTime      MinMapTime      AvgMapTime      MedianMapTime   MaxReduceTime   MinReduceTime   AvgReduceTime   MedianReducetime        Alias   Feature Outputs\n",
    "    job_1459245605938_0141  1492    101     101     20      53      58      9179    18      186     29      ntriples,raw,subject_frequencies,subject_groups GROUP_BY,COMBINER\n",
    "    job_1459245605938_0142  7       1       55      39      47      49      15      15      15      15      out_degree_groups,out_degree_sequence   GROUP_BY,COMBINER       /user/user2/out_degree_sequence,\n",
    "\n",
    "    Input(s):\n",
    "    Successfully read 460000000 records (536497 bytes) from: \"s3n://hw2storage/*\"\n",
    "\n",
    "    Output(s):\n",
    "    Successfully stored 1250 records (8652 bytes) in: \"/user/user2/out_degree_sequence\"\n",
    "\n",
    "    Counters:\n",
    "    Total records written : 1250\n",
    "    Total bytes written : 8652\n",
    "    Spillable Memory Manager spill count : 0\n",
    "    Total bags proactively spilled: 6\n",
    "    Total records proactively spilled: 456922\n",
    "\n",
    "    Job DAG:\n",
    "    job_1459245605938_0141  ->      job_1459245605938_0142,\n",
    "    job_1459245605938_0142\n",
    "\n",
    "\n",
    "    16/04/10 17:08:32 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-8-18.eu-west-1.compute.internal/172.31.8.18:8032\n",
    "    16/04/10 17:08:32 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
    "    16/04/10 17:08:34 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-8-18.eu-west-1.compute.internal/172.31.8.18:8032\n",
    "    16/04/10 17:08:34 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
    "    16/04/10 17:08:34 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-8-18.eu-west-1.compute.internal/172.31.8.18:8032\n",
    "    16/04/10 17:08:34 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
    "    16/04/10 17:08:34 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-8-18.eu-west-1.compute.internal/172.31.8.18:8032\n",
    "    16/04/10 17:08:34 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
    "    16/04/10 17:08:34 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-8-18.eu-west-1.compute.internal/172.31.8.18:8032\n",
    "    16/04/10 17:08:34 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
    "    16/04/10 17:08:34 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-8-18.eu-west-1.compute.internal/172.31.8.18:8032\n",
    "    16/04/10 17:08:34 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
    "    12126415 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - Success!\n",
    "    16/04/10 17:08:34 INFO mapReduceLayer.MapReduceLauncher: Success!\n",
    "    12126449 [main] INFO  org.apache.pig.Main  - Pig script completed in 3 hours, 22 minutes, 6 seconds and 594 milliseconds (12126594 ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that Pig created 2 map reduce jobs, just like in our execution.\n",
    "\n",
    "The first job creates 1,492 map and 101 reduce tasks. The second jobs creates 7 map and 1 reduce task. The total execution time is 3 hours and 22 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the results to the ones from Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are equal: True\n"
     ]
    }
   ],
   "source": [
    "resultC = {}\n",
    "path = 'partC/out_degree_sequence'\n",
    "file_list = [os.path.join(path,file) for file in os.listdir(path) if os.path.isfile(os.path.join(path,file)) and file != '_SUCCESS']\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as fin:\n",
    "        for line in fin:\n",
    "            key, value = line.split()\n",
    "            resultC[key] = value\n",
    "            \n",
    "equal = False\n",
    "if(resultA == resultC):\n",
    "    equal = True\n",
    "print 'Results are equal: ' + str(equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are again equal, as expected, so we don't need to plot the graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query to calculate the out-degree-sequence is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SELECT out_degree, COUNT(*) AS out_degree_sequence\n",
    "\n",
    "    FROM (\n",
    "        SELECT subject, COUNT(*) AS out_degree\n",
    "        FROM rdf \n",
    "        GROUP BY subject) AS out_a\n",
    "    GROUP BY out_degree;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the EXPLAIN ANALYZE function we get statistics concerning the executed query:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    EXPLAIN ANALYZE\n",
    "    SELECT out_degree, COUNT(*) AS out_degree_sequence\n",
    "    FROM (\n",
    "        SELECT subject, COUNT(*) AS out_degree\n",
    "        FROM rdf \n",
    "        GROUP BY subject) AS out_a\n",
    "    GROUP BY out_degree;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                                                                                     QUERY PLAN                                                                                     \n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    " Gather Motion 4:1  (slice2; segments: 4)  (cost=213061.59..213074.09 rows=1000 width=16)\n",
    "   Rows out:  1250 rows at destination with 1021366 ms to first row, 1021369 ms to end, start offset by 29 ms.\n",
    "   ->  HashAggregate  (cost=213061.59..213074.09 rows=250 width=16)\n",
    "         Group By: out_a.out_degree\n",
    "         Rows out:  Avg 312.5 rows x 4 workers.  Max 319 rows (seg3) with 1021364 ms to first row, 1021365 ms to end, start offset by 30 ms.\n",
    "         Executor memory:  4201K bytes avg, 4201K bytes max (seg0).\n",
    "         ->  Redistribute Motion 4:4  (slice1; segments: 4)  (cost=213026.59..213046.59 rows=250 width=16)\n",
    "               Hash Key: out_a.out_degree\n",
    "               Rows out:  Avg 808.0 rows x 4 workers at destination.  Max 830 rows (seg3) with 867122 ms to first row, 1021357 ms to end, start offset by 36 ms.\n",
    "               ->  HashAggregate  (cost=213026.59..213026.59 rows=250 width=16)\n",
    "                     Group By: out_a.out_degree\n",
    "                     Rows out:  Avg 808.0 rows x 4 workers.  Max 822 rows (seg3) with 867049 ms to first row, 867050 ms to end, start offset by 37 ms.\n",
    "                     Executor memory:  4201K bytes avg, 4201K bytes max (seg0).\n",
    "                     ->  Subquery Scan out_a  (cost=207401.41..212003.83 rows=51138 width=8)\n",
    "                           Rows out:  Avg 2874612.0 rows x 4 workers.  Max 3128256 rows (seg2) with 988902 ms to first row, 1020252 ms to end, start offset by 42 ms.\n",
    "                           ->  HashAggregate  (cost=207401.41..209958.31 rows=51138 width=40)\n",
    "                                 Group By: rdf.subject\n",
    "                                 Rows out:  Avg 2874612.0 rows x 4 workers.  Max 3128256 rows (seg2) with 988902 ms to first row, 1018753 ms to end, start offset by 42 ms.\n",
    "                                 Executor memory:  50528K bytes avg, 51040K bytes max (seg1).\n",
    "                                 Work_mem used:  42601K bytes avg, 42601K bytes max (seg0). Workfile: (4 spilling, 0 reused)\n",
    "                                 Work_mem wanted: 339792K bytes avg, 365609K bytes max (seg2) to lessen workfile I/O affecting 4 workers.\n",
    "                                 (seg2)   3128256 groups total in 32 batches; 1 overflows; 15674600 spill groups.\n",
    "                                 (seg2)   Hash chain length 1.5 avg, 13 max, using 2326981 of 4325376 buckets.\n",
    "                                 ->  Seq Scan on rdf  (cost=0.00..157623.27 rows=2488907 width=61)\n",
    "                                       Rows out:  Avg 115000000.0 rows x 4 workers.  Max 134428969 rows (seg0) with 0.499 ms to first row, 907688 ms to end, start offset by 44 ms.\n",
    " Slice statistics:\n",
    "   (slice0)    Executor memory: 267K bytes.\n",
    "   (slice1)  * Executor memory: 54895K bytes avg x 4 workers, 55407K bytes max (seg1).  Work_mem: 42601K bytes max, 365609K bytes wanted.\n",
    "   (slice2)    Executor memory: 4451K bytes avg x 4 workers, 4451K bytes max (seg0).\n",
    " Statement statistics:\n",
    "   Memory used: 128000K bytes\n",
    "   Memory wanted: 1097224K bytes\n",
    " Optimizer status: legacy query optimizer\n",
    " Total runtime: 1021398.511 ms\n",
    "(34 rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the results above, the query was executed in 1,021 seconds, that is a bit more than 17 minutes. And the results where again the same as with previous executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-0cc8a575cdef>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-0cc8a575cdef>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    rdfdb=> SELECT gp_segment_id, count(*)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "rdfdb=> SELECT gp_segment_id, count(*)\n",
    "FROM rdf         \n",
    "GROUP BY gp_segment_id;\n",
    "\n",
    " gp_segment_id |   count   \n",
    "---------------+-----------\n",
    "             3 |  96079527\n",
    "             1 |  95162209\n",
    "             2 | 134329295\n",
    "             0 | 134428969\n",
    "(4 rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the query above we observer that the segments do not have the same amount of data. Segments 0 and 2 have significantly more rows than segments 1 and 3. The data is distributed by hashing the subject field, this is why the distribution is not equal as if it were distributed using a random (round-robin) process.\n",
    "\n",
    "The hashing distribution uses a field of hte data as input to a hash algorithm. The hash output determines the segment where the data will be stored. This guarantees that rows with the same value in the field being hashed will reside in the same segment.\n",
    "\n",
    "On the other hand the random distribution, assigns the data to the segments in a round robin function as they are inserted into the system. We believe that this distribution would not be beneficial tou our queries. We use group by to count the frequencies of subjects and it is beneficial if all the entries concerning one sbject are in one segment to avoid inter-segment communication.\n",
    "\n",
    "Partitioning a table using the PARTITION BY (AND SUBPARTITION BY) clauses during table creation, breaks the table to smaller ones based on the values of the fields of the partition. For example a field representing date can be used to partition a table where each subtable will have data from a date range. Another partition field could be a field representing the country name, where each country would get a different table. This way an hierarchy of tables is created with the goal to increase performance in queries that need to scan only a subset of the values. For example if a table is partitioned based on a \"Year\" field, queries about transactions of a year will nt need to scan a large table concerning multiple years but will only scan the (smaller) table with the data of that year.\n",
    "\n",
    "Distribution on the other hand defines the way that data of a given table (or subtable) are asigned to segments. These are different and non overlaping concepts. A partitioned table can be distributed in different segments. Each segment will have as many subtables as needed to store the data.\n",
    "\n",
    "In our case the table is not partitioned. But as we need to scan all the table to get result a partition would not speed up the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MariaDB [babis]> LOAD DATA LOCAL INFILE '/home/babis/git/M36103P/assignment2/data/hw2-rdf-2016_2' into table test fields terminated by ' ';\n",
    "Query OK, 10000000 rows affected, 65535 warnings (7 min 0.89 sec)\n",
    "Records: 10000000  Deleted: 0  Skipped: 0  Warnings: 1000106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the query above we inserted 1 of the 46 files in a traditional SQL database (MariaDB, a drop in replacement for MySQL). In the traditional databases much of the time needed for insertion is spent on validating the data and updating the indices. 1 file containing 10,000,000 rows needed 7 minutes. As the indices are most probably trees, inserting a new value is O(logn), thus the time needed for each insertion will increase as the table grows. With this in mind we estimate that several hours (10?) would be needed to inserted all the data in the database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
